\section{Results}
\label{sec:results}

%Performance Evaluation, Results, Discussion}

%\textit{owners: Evan: graphs, All: interpretation (2.75 pages)}

\subsection{CX Performance on Single Node}
  \label{sxn:results1}


   
  \vspace*{0.1in}

      In Table~\ref{tab:single_node}, we show the benefits of various
      optimizations described in
      Sec.~\ref{sxn:single_node_opt} on a single-node of our Spark system. 
      The test matrix $\mathcal{A}$ has {\it{m}} = 1.95M, {\it{n}} = 128K,
      {\it{s}} = 0.004, and {\it{nnz}} = 10$^9$. The rank parameter,
      {\it{k}} = 32. We started with a parallelized implementation,
      without any of the described optimizations, and measured the
      performance (in terms of time taken). We first implemented the
      multi-core synchronization scheme, wherein a single copy of the
      output matrix is maintained across all the threads (for the matrix multiplication).
      This resulted in a speedup of around 6.5X, primarily due to
      the reduction in the amount of data traffic between the
      last-level cache and main memory (there was around 19X measured reduction
      in traffic). We then implemented our cache blocking scheme,
      primarily targeted towards ensuring that the output of the
      matrix multiplication resides in the caches (since it is
      accessed and updated frequently). This led to a further 2.4X
     reduction in run-time, for an overall speedup of around 15.6X.

     Once the memory traffic was optimized for, we implemented our
     SIMD code, by vectorizing the element-row multiplication-add
     operations (described in detail in Sec.~\ref{sxn:single_node_opt}). 
     The resultant code sped up by a further 2.6X, for an overall
     speedup of 39.7X. Although the effective SIMD width
 ($\mathcal{S}$ = 4), there are overheads of address computation,
 stores, and not all computations were vectorized (QR code is still
 scalar).


 
  \begin{table}
  \begin{center}
  \begin{tabular}{ |c|c| } 
  \hline
  Single Node Optimization & Overall Speedup\\
  \hline
  Original Implementation & 1.0  \\
  Multi-Core Synchronization & 6.5 \\
  Cache Blocking & 15.6 \\
  SIMD & 39.7 \\
  \hline

  \end{tabular}
  \end{center}
  \caption{Single node optimizations to the CX C implementation and
  the subsequent speedup  each additional optimization provides.}
  \label{tab:single_node}
  \end{table}
 



  \subsection{CX Performance across Multiple Nodes}
%  \textcolor{red}{Mike R, Jatin: we need a narrative here}

  \subsubsection{CX Spark Phases}
  Our implementations of CX and PCA share the \textsc{RandomizedSVD} subroutine, which accounts for the bulk of the runtime and all of the distributed computations.
  The execution of \textsc{RandomizedSVD} proceeds in four distributed phases listed below, along with a small amount of additional local computation.
  \begin{enumerate}
      \item \textbf{Load Matrix Metadata}
         The dimensions of the matrix are read from the distributed filesystem to the driver.
      \item \textbf{Load Matrix}
         A distributed read is performed to load the matrix entries into an in-memory cached
         RDD containing one entry per row of the matrix.
      \item \textbf{Power Iterations}
         The \textsc{MultiplyGramian} loop on lines 2-5 of
         \textsc{RandomizedSVD} is run to compute an approximation $Q$
         of the dominant right singular subspace.
       \item \textbf{Finalization (Post-Processing)}
         Right multiplication by $Q$ on line 7 of \textsc{RandomizedSVD} to compute $C$.
  \end{enumerate}

  \subsubsection{Empirical Results}

    \begin{figure} [H]
    \begin{centering}
    \includegraphics[scale=0.4]{images/CX_Strong_Scaling_Rank_32_Partitions_default.pdf}
    \end{centering}
    \caption{ Strong scaling for the 4 phases of CX on an XC40 for 100GB dataset at rank 32 and default partitioning as concurrency is increased.} 
    \label{fig:xc40scaling}
    \end{figure} 

Figure~\ref{fig:xc40scaling} shows how the distributed Spark portion of our code scales as we add additional processors.  We considered 240, 480, and 960 cores.  An additional doubling (to 1920 cores) would be ineffective as there are only 1654 partitions, so many cores would remain unused.  In addition, with fewer partitions per core there are less opportunities for load balancing and speculative reexecution of slow tasks.

When we go from 240 to 480 cores, we achieve a speedup of approximately 1.6x from doubling the cores: 233 seconds versus 146 seconds.  However, as the number of partitions per core drops below two, and the amount of computation-per-core relative to communication overhead drops, the scaling slows down (as expected).  This results in a lower speedup of approximately 1.4x (146 seconds versus 102 seconds) when we again double the core count to 960.

  \subsection{CX Performance across Multiple Platforms}
  \label{sect:h2h}
    
    \begin{figure} [H]
    \begin{centering}
    \includegraphics[scale=0.4]{images/CX_Size_Scaling_Rank_16_Partitions_default.pdf}
    \end{centering}
    \caption{ Run times for the various stages of computation for CX for two different dataset sizes for the three platforms using rank 16 and default partitioning for the given platform} 
    \label{fig:h2hrank16} 
    \end{figure}

    
  \input{h2hresults.tex}

  
  \subsection{Timing and Accuracy comparison of RSVD, CX, and truncated SVD}

 Because the RSVD allows us to explicitly control the number of matrix-vector
 products, the accuracy of the low-rank approximation obtained from the RSVD
 algorithm is expected to be somewhat lower than that of truncated SVD
 approximation. Similarly, CX decompositions have the advantage of
 interpretability, but come at the cost of an increased number of operations on
 top of the RSVD and an additional loss in approximation accuracy. 

  In Figures~\ref{fig:timing-accuracy-8} and~\ref{fig:timing-accuracy-16}, we observe the timing vs accuracy tradeoffs of the RSVD and CX algorithms
  as applied to the 100G MSI dataset for two settings of the rank parameter, $k=8$ and $k=16.$ The exact SVD was computed in this case using the
  Spark bindings of the popular ARPACK eigenproblem library~\cite{ArpackUserGuide}. The RSVD algorithm used two power iterations, and we used the output of the RSVD algorithm to generate
  both the column CX decomposition defined in Algorithm~\ref{alg:cx} and a related 'row CX' decomposition that comes from applying Algorithm~\ref{alg:cx}
  to $A^T.$ As explained in more detail in the next section, both of these CX decompositions are of interest, as they identify important pixels and ions.

  For both rank parameters, we observe the behavior predicted by the theory for the RSVD decomposition: the approximation error is only slightly greater than that of the 
  truncated SVD approximation. We also observe that there is only a slight speed advantage to using the RSVD; this is likely attributable to the fact that the input matrix
  is truly low-rank (more than 70\% of the Frobenius norm of the matrix is already captured by the rank-8 decomposition), so the iterative ARPACK SVD algorithm converges 
  quite fast.

  The CX decompositions require significantly more time to compute than the truncated SVD and RSVD decompositions. This is due to the need to compute the projection of $A$ onto
  the columns $C$ after $C$ is constructed according to Algorithm~\ref{alg:cx}. We also note that for the MSI dataset, row-based CX decompositions are more 
  accurate and less expensive to construct than column-based CX decompositions. 

  \begin{figure}[h!btp]
    \begin{centering}
      \includegraphics[scale=0.4]{images/timing-accuracy-8}
      \end{centering}
      \caption{The Frobenius norm approximation errors and timings for three runs of the RSVD and CX approximations relative to those of the truncated SVD for a target rank of $k=8$ on the 100G MSI dataset.}
    \label{fig:timing-accuracy-8}
  \end{figure}

  \begin{figure}[h!btp]
    \begin{centering}
      \includegraphics[scale=0.4]{images/timing-accuracy-16}
      \end{centering}
      \caption{The Frobenius norm approximation errors and timings for three runs of the RSVD and CX approximations relative to those of the truncated SVD for a target rank of $k=16$ on the 100G MSI dataset.}
    \label{fig:timing-accuracy-16}
  \end{figure}

  \subsection{Science Results}
  \subsubsection{CX}
  Recall that CX decomposition samples actual columns from $A$ for low-rank approximation. In other words, it allows us to identify important columns based on which $A$ can be well reconstructed. Here, we examine the results of CX decomposition, i.e., columns sampled, on the MSI dataset.
 Rows and columns of our data matrix $A$ are corresponding to spacial pixels and $(\tau, m/z)$ value of ions, respectively. We apply CX decomposition on both $A$ and $A^T$ in order to identify important pixels and ions.
   
  In Figure~\ref{fig:cx_ions}, we present the distribution of the marginalized normalized ion leverage scores over $\tau$. That is, each score corresponds to an ion with $m/z$ value shown in the $x$-axis. Clearly, leverage scores of ions in three narrow regions have significantly larger magnitude than the rest. This indicates that these ions are more informative and should be kept as basis for reconstruction.  {\color{red} XXX: BEN, PLEASE COMMENT ON THE EXACT IONS FOUND. JEY MIGHT HAVE SENT YOU THE LIST M/Z VALUES. }
  
  On the other hand, we observed that the pixel leverage scores are fairly uniform (not shown here). This is not surprising since similar pixels tend to have similar and small individual scores. However, for each region that contains similar pixels, the total leverage score (sampling probability) will still be higher if such a region is more important in the reconstruction. Therefore, a random sampling is still able to capture the importance by sampling more pixels from the region as shown in Figure~\ref{fig:cx_spatial}.
  {\color{red} XXX: BEN, PLEASE COMMENT ON THESE REGIONS. }
    
    \begin{figure} [h!btp]
    \begin{centering}
    \includegraphics[width=\linewidth]{images/cx_ions.png}
    \end{centering}
    \caption{Normalized leverage scores (sampling probabilities) for $m/z$ marginalized over $\tau$.
      Three narrow regions of $m/z$ account for $59.3\%$ of the total probability mass.}
    \label{fig:cx_ions}
    \end{figure} 
    
    \begin{figure} [h!btp]
    \begin{centering}
    \includegraphics[width=\linewidth]{images/cx_spatial.png}
    \end{centering}
    \caption{Plot of 10000 points sampled by leverage score. Color and
      luminance of each point indicates density of points at that location as
      determined by a Gaussian kernel density estimate.}
    \label{fig:cx_spatial}
    \end{figure} 


  \subsection{Discussion}
  \label{sect:lessons}
  
  \input{lessons.tex}

